---
tags:
  - LargeLanguageModels
---
## Introduzione
La cosine similarity √® una misura del grado di similarit√† tra due vettori in uno spazio multidimensionale.
Viene comunemente utilizzata nell'intelligenza artificiale e nell'elaborazione del linguaggio naturale per confrontare [[Embeddings]], che sono rappresentazioni numeriche di parole o altri oggetti in modo indipendente dalla loro lunghezza e dimensione.
La similarit√† cosinusoidale tra due vettori viene calcolata prendendo il prodotto scalare dei due vettori e dividendo il risultato per il prodotto delle loro magnitudini. Ci√≤ produce un valore compreso tra -1 e 1, dove 1 indica che i due vettori sono identici, 0 indica che sono ortogonali (cio√® non hanno correlazione) e -1 indica che sono opposti.
La similarit√† cosinusoidale √® particolarmente utile quando si lavora con dati ad alta dimensionalit√†, come gli [[embeddings|embedding]] di parole, perch√© tiene conto sia della magnitudine che della direzione di ogni vettore. Ci√≤ la rende pi√π robusta rispetto ad altre misure come la distanza euclidea, che considera solo la magnitudine.

## Calcolo della similarit√†
Per calcolare la similarit√† del coseno tra due vettori, √® necessario eseguire i seguenti passaggi:
1. Normalizzare i vettori: √® importante normalizzare i vettori per rimuovere le differenze di lunghezza. Questo pu√≤ essere fatto dividendo ciascun vettore per la sua norma euclidea.
2. Calcolare il prodotto scalare: moltiplicare i vettori componente per componente e sommare i risultati.
3. Calcolare le norme dei vettori: calcolare la norma euclidea dei due vettori.
4. Applicare la formula della similarit√† del coseno: dividere il prodotto scalare ottenuto per il prodotto delle norme dei vettori.

La formula per calcolare la similarit√† del coseno tra due vettori ùëé e ùëè √® la seguente:
```
similarita' = ùëé ‚Ä¢ ùëè / (‚Äñùëé‚Äñ √ó ‚Äñùëè‚Äñ)
```

## Casi d'uso

* **Parole simili**. Ad esempio, dato un embedding per "gatto", possiamo utilizzare la cosine similarity per trovare altre parole con embeddings simili, come "gattino" o "felino". Ci√≤ pu√≤ essere utile per compiti come la classificazione del testo o l'analisi del sentiment, dove vogliamo raggruppare parole semanticamente correlate.
* **Sistemi di raccomandazione**: Rappresentando gli elementi (ad esempio, film, prodotti) come vettori, possiamo utilizzare la similarit√† cosinusoidale per trovare elementi simili tra loro o a un particolare elemento di interesse. Ci√≤ ci consente di fornire raccomandazioni personalizzate basate sul comportamento o sulle preferenze passate dell'utente.
* **Riconoscimento di immagini**: la similarit√† cosinusoidale pu√≤ essere utilizzata per confrontare gli incorporamenti di due immagini, il che pu√≤ aiutare nei compiti di riconoscimento delle immagini.  
- **Elaborazione del linguaggio naturale**: la similarit√† cosinusoidale pu√≤ essere utilizzata per misurare la similarit√† semantica tra due frasi o paragrafi confrontando i loro vettori di incorporamento.    
- **Clustering**: la similarit√† cosinusoidale pu√≤ essere utilizzata come metrica di distanza per algoritmi di clustering, aiutando a raggruppare insieme punti dati simili.    
- **Rilevamento di anomalie**: la similarit√† cosinusoidale pu√≤ essere utilizzata per identificare anomalie in un [[dataset]] trovando punti dati che hanno una bassa similarit√† cosinusoidale con gli altri punti dati nel dataset.

Nel complesso, la similarit√† cosinusoidale √® uno strumento essenziale per gli sviluppatori che lavorano con intelligenza artificiale e embeddings. La sua capacit√† di catturare sia la magnitudine che la direzione la rende adatta per dati ad alta dimensionalit√†, e le sue applicazioni nell'elaborazione del linguaggio naturale e nei sistemi di raccomandazione la rendono uno strumento prezioso per la creazione di applicazioni intelligenti.
